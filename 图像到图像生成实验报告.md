# 图像到图像生成实验报告

1120230512王子铭 1120231952贾箫然 1120233630 赖宇琦

## 1. 实验背景与目标

图像到图像转换（Image-to-Image Translation）旨在学习从一种视觉域（如语义标签、边缘、深度图、素描）到另一种视觉域（如真实图像、彩色渲染、风格图像）的映射关系。本实验以 Cityscapes 城市街景数据为例，从语义标签图生成真实街景照片，并探索不同模型在**结构保持**与**真实感生成**之间的平衡。

本实验目标包括：理解生成模型训练机制；完成 Label→Photo 任务；探索不同损失项对生成质量的影响；掌握并对比 PSNR/SSIM/MAE/FID 等指标并进行可视化分析。

## 2. 数据集与预处理

### 2.1 数据集说明

使用任务书提供的 Cityscapes 格式拼接图像数据：**每张图片左侧为真实街景 photo，右侧为语义标签 label**。
数据集下载地址：https://gitcode.com/open-source-toolkit/04615。

### 2.2 预处理流程

1. **拆分拼接图**：按图像宽度中线水平切分，左半为 `photo`，右半为 `label`（以宽度一分为二裁剪）。若原始图像宽度为奇数，则以向下取整的方式切分。
2. **颜色空间**：统一以 `RGB` 读入并处理两半图，保持通道一致（3 通道，连续值）。
3. **尺寸统一**：两半图均 `Resize` 到 `256×256`，不保持纵横比（以加速训练与统一网络输入）。
4. **张量化**：转换为张量，范围 `[0,1]`，类型 `float32`，通道顺序 `C×H×W`。确保两半图使用完全相同的变换序列，以维持像素级对齐。
5. **归一化**：对三通道使用 `mean=(0.5,0.5,0.5)` 与 `std=(0.5,0.5,0.5)`，使张量范围约为 `[-1, 1]`。该范围与生成器输出一致，便于训练与推理。
6. **反归一化（用于保存/评估）**：在保存或评估指标（PSNR/SSIM/MAE/FID）前，将图像从 `[-1,1]` 映射回 `[0,1]`，例如：`x_vis = (x + 1) / 2`；若采用一般化 `Normalize/Denormalize`，则按 `x_vis = x * std + mean` 还原。
7. **返回形式**：每个样本返回 `(label, photo)` 二元组，两者预处理完全一致，便于以 `label` 为条件、`photo` 为监督目标进行训练与评估。

## 3. 实验环境与依赖

本实验实际环境：

- OS：`Windows 11 家庭中文版`
- Python：`3.12`
- PyTorch：`2.8.0`
- CUDA / GPU：`rtx 5070ti laptop`
- 训练时长：`前后总计约100h`

## 4. 方法与模型设计

本实验统一任务方向为 **Label → Photo**，训练过程中每个 epoch 保存验证集**三联图（Label / Generated / Ground Truth）**并记录指标用于对比分析。

### 4.1 U-Net（监督基线）

**思想**：把问题当作标准的像素到像素回归/翻译。输入为语义标签图，输出为真实街景图。

**网络结构**：
标准 Encoder-Decoder 架构，包含 Skip Connections：

- **Encoder**（下采样）：5 个下采样块，通道逐步增加（64 → 128 → 256 → 512 → 1024）
- **Decoder**（上采样）：5 个上采样块，通道逐步减少（1024 → 512 → 256 → 128 → 64）
- **Skip Connections**：连接对应分辨率的 Encoder 和 Decoder 特征，保持空间信息
- 每个卷积块采用 `Conv2d → BatchNorm2d → ReLU` 的标准结构
- 最终输出层：`Conv2d(64, 3, kernel_size=7)` 生成 RGB 图像，激活函数采用双曲正切（Tanh）映射到 [-1, 1]

**损失函数**：

以 L1 像素级重建损失为主：
$$\mathcal{L}_{L1} = \mathbb{E}_{(x,y)} \left[|G(x)-y|_1\right]$$

可选补充感知损失（Perceptual Loss）以提高高频细节：
$$\mathcal{L}_{perc} = \sum_{l} \lambda_l \mathbb{E}[\|f_l(G(x)) - f_l(y)\|_1]$$
其中 $f_l$ 为预训练 VGG19 网络第 $l$ 层的特征提取函数。

**预期特点**：

- 结构对齐良好（Skip Connections 保留空间细节）
- 可能出现"平均化"效应导致纹理不够真实、细节偏糊
- 尤其在车灯、路牌、建筑纹理等高频细节处理能力有限

### 4.2 Pix2Pix + U-Net（条件 GAN）

**目标**：在 L1 监督约束结构对齐的同时，引入对抗学习提高真实感。
**损失**：

对抗损失（LSGAN，MSE）：
$$\mathcal{L}_{cGAN}(G,D) = \mathbb{E}_{x,y}[\|D(x,y)-1\|^2] + \mathbb{E}_x[\|D(x,G(x))\|^2]$$

L1 重建损失：
$$\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y}[|y-G(x)|_1]$$

总损失：
$$\mathcal{L}(G,D) = \mathcal{L}_{cGAN}(G,D) + \lambda_{L1} \cdot \mathcal{L}_{L1}(G), \quad \lambda_{L1} = 100$$

**预期特点**：相较纯 U-Net，纹理更锐利、更“像真图”，但训练更不稳定，可能出现局部伪影/噪点。

### 4.3 CycleGAN（无配对/弱配对思路）

CycleGAN 是 Pix2Pix 后的重要变体。虽然本任务数据天然配对（label-photo ），但可以使用 CycleGAN 作为对照方案：学习两个互逆生成器 $G: \text{Label} \to \text{Photo}$ 和 $F: \text{Photo} \to \text{Label}$，通过循环重建保持内容一致性。

**网络结构**：

- **生成器**（×2）：ResNet 风格生成器，针对 256×256 输入优化
  - 初始卷积：1 层（7×7，64 通道）
  - 下采样：2 层（3×3，步长 2，128→256 通道）
  - 残差块：9 个（保持 256 通道，Instance Normalization）
  - 上采样：2 层（转置卷积，步长 2）
  - 最后卷积：1 层（7×7，输出 3 通道，Tanh 激活）
- **判别器**（×2）：70×70 PatchGAN，4 层卷积，步长 2，通道：64 → 128 → 256 → 512

**损失函数**：

对抗损失（LSGAN）：
$$\mathcal{L}_{GAN}(G,D_Y) = \mathbb{E}_{x}[\|D_Y(G(x))-1\|^2] + \mathbb{E}_{y}[\|D_Y(y)\|^2]$$

循环一致性：
$$\mathcal{L}_{cyc}(G,F) = \mathbb{E}_x[|F(G(x))-x|_1] + \mathbb{E}_y[|G(F(y))-y|_1]$$

恒等损失：
$$\mathcal{L}_{id}(G,F) = \mathbb{E}_y[|G(y)-y|_1] + \mathbb{E}_x[|F(x)-x|_1]$$

总损失：$\mathcal{L} = \mathcal{L}_{GAN}(G,D_Y) + \mathcal{L}_{GAN}(F,D_X) + \lambda_{cyc} \cdot \mathcal{L}_{cyc} + \lambda_{id} \cdot \mathcal{L}_{id}$，其中 $\lambda_{cyc} = 10.0$，$\lambda_{id} = 5.0$。

**预期特点**：在无严格配对设置下可训练；但对强结构任务易出现语义错配、边界不清；循环重建导致路面伪影；整体成本高（2G + 2D），收敛慢。

## 5. 训练设置与实现细节

### 5.1 三模型对比（U-Net vs Pix2Pix + U-Net vs CycleGAN）

- 输入输出：Label → Photo
- U-Net：
  - batch size：8
  - 学习率：2e-4（lr decay：线性衰减，起始 epoch=50）
  - 优化器：Adam（betas=(0.5, 0.999)）
  - epoch 数：100
  - 损失权重：λ_L1=100.0，λ_perceptual=10.0，λ_GAN=1.0（GAN 模式：vanilla BCE）
- Pix2Pix + U-Net：
  - batch size：8
  - 学习率：2e-4（lr decay：线性衰减，后半程开始）
  - 优化器：Adam（betas=(0.5, 0.999)）
  - epoch 数：200
  - 损失权重：λ_L1=100.0（GAN 模式：LSGAN，MSE）
- CycleGAN：
  - batch size：1（Windows 下 num_workers=0，pin_memory=True）
  - 学习率：2e-4（lr decay：无）
  - 优化器：Adam（betas=(0.5, 0.999)）
  - epoch 数：100
  - 损失权重：λ_cyc=10.0，λ_id=5.0（GAN 模式：LSGAN，MSE）
- 保存与可视化：每个 epoch 保存验证集三联图（Label / Generated / GT）并记录 PSNR、SSIM、MAE、FID。

### 5.2 损失函数对比

本实验中，我们还进行了对比不同损失项与扩展采样策略，具体如下。

**方法概览**

- L1（监督回归）：`U-Net` 以 L1 重建损失训练，侧重像素一致性与结构对齐。
- Pix2Pix（cGAN + L1）：`U-Net` 生成器 + `PatchGAN` 判别器，对抗损失增强细节与真实感，联合 L1 稳定结构。
- Diffusion（DDPM/DDIM）：`SimpleUNet` 噪声预测；训练用噪声 MSE，推理支持 `DDPM` 或更快的 `DDIM`；含余弦调度与 `Classifier-Free Guidance（CFG）`；可选 `x0` 辅助 L1 稳定早期训练。

**损失与采样设置**

- L1：$\mathcal{L}_{L1}=\mathbb{E}_{x,y}[\,|G(x)-y|_1\,]$。
- Pix2Pix：$\mathcal{L}=\mathcal{L}_{cGAN}(G,D)+\lambda\,\mathcal{L}_{L1}(G)$，$\lambda$ 取 `100`。
- Diffusion 训练：噪声预测 MSE；可选 $\mathcal{L}_{x0}=\|\hat{x}_0-y\|_1$（权重可调），`beta` 采用余弦调度。
- Diffusion 采样：`DDPM`（标准后验），`DDIM`（确定性/半确定性、步数更少），`CFG`（以概率丢弃条件，用 `guidance_scale` 放大条件引导）。

**评估协议**

- 指标：PSNR、SSIM、MAE、FID。
- 可视化：每个 epoch 保存验证样例三联图（Label / Generated / GT）。
- 数据：使用 [dataset/train](dataset/train) 与 [dataset/val](dataset/val)，拼接图按中线水平拆分。

## 6. 评价指标

本实验使用以下指标并结合主观观察来评价生成图片好坏。

### 6.1 PSNR（Peak Signal-to-Noise Ratio，峰值信噪比）

**衡量什么**：生成图与真实图在**像素层面**的误差大小（基于均方误差 MSE）。PSNR 越高表示像素越接近。

**定义与公式**：
先计算均方误差：
$$
MSE=\frac{1}{HWC}\sum_{i=1}^{H}\sum_{j=1}^{W}\sum_{k=1}^{C}\left(x_{ijk}-y_{ijk}\right)^2
$$
再计算 PSNR：
$$
PSNR = 10\log_{10}\left(\frac{MAX_I^2}{MSE}\right)
$$
其中 ($MAX_I$) 是像素最大取值（例如 255 或 1）。

**优点**

- 计算简单、稳定，适合做训练过程的量化跟踪。
- 对整体像素误差敏感，能够反映“是否接近原图”的程度。

**缺点**

- 与人眼感知相关性有限：可能出现“PSNR 高但视觉偏糊”的情况（例如输出更平滑时 MSE 变小）。
- 对轻微错位/小位移非常敏感：即使视觉上差别不大，PSNR 也可能明显下降。

### 6.2 SSIM（Structural Similarity Index，相似结构指数）

**衡量什么**：两张图在**亮度、对比度、结构信息**上的相似程度，更贴近人眼对结构与纹理的感知。

**定义与公式**（常在局部窗口内计算）：
$$
SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}
$$
其中

- ($\mu_x$,$\mu_y$)：局部均值（亮度）
- ($\sigma_x$,$\sigma_y$)：局部标准差（对比度）
- ($\sigma_{xy}$)：协方差（结构相关）
- ($C_1$,$C_2$)：稳定常数（防止分母接近 0）

**取值范围**：通常在 ([0,1])，越接近 1 越好（实现细节不同可能略有变化）。

**优点**

- 比像素误差指标更能反映结构保真度（边缘、轮廓、局部纹理一致性）。
- 对整体亮度的小幅变化相对更鲁棒。

**缺点**

- 仍依赖空间对齐：若对象位置偏移或边界错位，SSIM 会显著降低。
- 对“整体结构像但局部纹理不自然”的情况未必敏感，可能出现与主观观感不一致。

### 6.3 MAE（Mean Absolute Error，平均绝对误差）

**衡量什么**：生成图与真实图**像素差的绝对值平均**，反映“平均每个像素差多少”。

**定义与公式**：
$$
MAE=\frac{1}{HWC}\sum_{i=1}^{H}\sum_{j=1}^{W}\sum_{k=1}^{C}\left|x_{ijk}-y_{ijk}\right|
$$
**优点**

- 相比 MSE/PSNR，对少量极端误差（离群点）更不敏感，数值更“平滑”。
- 解释直观：平均像素偏差大小。

**缺点**

- 与感知真实感关联有限：可能“MAE 低但画面缺乏真实纹理、细节偏平”。
- 同样对几何错位敏感：轻微位移会使 MAE 增大。

### 6.4 FID（Fréchet Inception Distance，弗雷歇距离）

**衡量什么**：生成图像集合与真实图像集合在**高层语义特征分布**上的距离，偏向评估“是否像真实照片”的感知质量。

**定义与公式**：
用预训练特征提取网络将图像映射到特征空间，假设真实特征与生成特征都近似服从高斯分布：

- 真实：(($\mu_r$,$\Sigma_r$))
- 生成：(($\mu_g$,$\Sigma_g$))

FID 定义为：
$$
FID = |\mu_r-\mu_g|_2^2 + \mathrm{Tr}\left(\Sigma_r+\Sigma_g-2(\Sigma_r\Sigma_g)^{1/2}\right)
$$
**优点**

- 更贴近人眼对“真实感/自然性”的判断，常用于生成任务的主流评价。
- 对纹理、颜色分布与语义特征差异较敏感。

**缺点**

- 统计依赖强：样本量太少会导致估计不稳定，结果波动较大。
- 对预处理与分辨率敏感：裁剪方式、缩放、归一化改变都可能影响 FID。
- 不能保证像素对齐正确：可能出现“整体像真但局部语义/位置不对”的情况，FID 仍可能不差。

## 7. 实验结果与分析

### 7.1 U-Net vs Pix2Pix + U-Net vs CycleGAN三模型定量指标对比

使用最后 10 个 epoch 的均值进行对比（PSNR/SSIM 越大越好；MAE/FID 越小越好；MAE 已做尺度对齐）

| 模型              | PSNR ↑      | SSIM ↑     | MAE(归一化) ↓ | FID ↓        |
| ----------------- | ----------- | ---------- | ------------- | ------------ |
| **Pix2Pix+U-Net** | **17.0745** | **0.5146** | **0.1036**    | 195.4991     |
| **CycleGAN**      | 13.3234     | 0.1925     | 0.1606        | **134.6187** |
| **UNet**          | 13.0677     | 0.3932     | 0.1523        | 335.8782     |

- **像素/结构一致性（PSNR/SSIM/MAE）**：**Pix2Pix+U-Net 明显最强**
  - 相比 CycleGAN：PSNR 约 **+28.2%**，SSIM 约 **+167%**，MAE 约 **-35.5%**
  - 相比 UNet：PSNR 约 **+30.7%**，SSIM 约 **+30.8%**，MAE 约 **-32.0%**
- **感知/分布一致性（FID）**：**CycleGAN 最好，其次 Pix2Pix+U-Net，UNet 最差**
  - 最佳 FID（全程最低点）：CycleGAN **112.29@epoch95** < Pix2Pix+U-Net **186.76@epoch21** < UNet **277.63@epoch83**

### 7.2 训练过程曲线（图）

[image-20251228122631707](C:\Users\wzm_d\AppData\Roaming\Typora\typora-user-images\image-20251228122631707.png)

![image-20251228122700722](C:\Users\wzm_d\AppData\Roaming\Typora\typora-user-images\image-20251228122700722.png)

![image-20251228122717972](C:\Users\wzm_d\AppData\Roaming\Typora\typora-user-images\image-20251228122717972.png)

![image-20251228122729242](C:\Users\wzm_d\AppData\Roaming\Typora\typora-user-images\image-20251228122729242.png)

插入你记录的曲线图（至少包含）：

- Generator / Discriminator loss（若有）
- PSNR、SSIM、MAE、FID 随 epoch 变化

并回答：

- 哪个模型收敛更快？是否出现震荡/模式崩溃迹象？
- FID 最佳 epoch 是否与 PSNR/MAE 最佳 epoch 不一致？（若不一致，说明“像素最优”和“感知最优”可能不同）

### 7.3 生成效果可视化（三联图）

#### 7.3.1CycleGAN

![epoch_100_val_296](C:\Users\wzm_d\Desktop\all\CycleGAN\results\epoch_100_val_296.png)

![epoch_100_val_65](C:\Users\wzm_d\Desktop\all\CycleGAN\results\epoch_100_val_65.png)

总体来看CycleGAN 输出呈现出**非常典型的不对齐特征：整体街景照片风格保留了，但语义位置和对象细节**跟输入 label 和 GT 对不上，且出现明显伪影。

具体总结如下：

##### 1) 语义不严格对齐

- 生成图都能给出“道路 + 建筑 + 树”的大块纹理与光照氛围，说明**对目标域的分布拟合**是有效的。
- 但**对象的精确位置/数量/形状**对不上：小目标（行人/骑行者/路牌/车道线）容易缺失或错位。

这可能与 CycleGAN 的核心特点有关：它更擅长**分布级的风格迁移**，不擅长这种“label→photo 需要强语义对齐的任务（因为它不利用配对监督去强行对齐每个像素/区域）。

##### 2) 细节发糊而且边界不干净

- 路缘、车轮廓、车道线这类**几何边界**都偏软、偏糊。
- 这种结果，可能是因为CycleGAN 只有对抗 + cycle 的约束，缺少 Pix2Pix 那种 **L1/L2 强配对重建**带来的效果。

##### 3) 路面出现脏纹理/裂纹感的高频伪影

- 两张都能看到路面出现不太自然的高频纹理。
- 这可以这么解释： CycleGAN 里**cycle consistency** 要求“photo→label→photo 可逆”，而 label 域信息很离散（大色块），模型有时会在 photo 里塞一些纹理信号来帮助反向还原，呈现出这种路面纹理。

> 总结：CycleGAN 能生成具有真实街景风格的整体纹理与光照，但在 label→photo 任务中容易出现语义错配与对象幻觉，小结构边界不清晰，并伴随路面高频伪影；因此更适合作为无配对风格迁移的对照模型，而不适合作为需要严格语义一致性的主方案。

#### 7.3.2 U-Net

![01bc54fb98726057851d4b06484cf6e8](D:\xwechat_files\wxid_dx8qi5swjz7l12_f13e\temp\RWTemp\2025-12\9e20f478899dc29eb19741386f9343c8\01bc54fb98726057851d4b06484cf6e8.png)

![fc41922bcfbe14f3b3c03f67aca60d7e](D:\xwechat_files\wxid_dx8qi5swjz7l12_f13e\temp\RWTemp\2025-12\fc41922bcfbe14f3b3c03f67aca60d7e.png)

总体来看，U-Net 的输出呈现出**稳定但偏平均/趋同**的特征：整体能生成“像街景照片”的外观，但对输入 label 的条件利用不足，导致**语义与细节**跟 Ground Truth 对不上，同时画面偏糊、纹理真实感有限。

具体总结如下：

##### 1) 条件信息利用不足，语义对齐不理想

- 两组样例中，与 Ground Truth 的关键内容不匹配：例如第 1 张 GT 中右侧明显车辆/路口标线等细节未在生成结果中对应出现；第 2 张 GT 场景更开阔且包含出租车等元素，但生成结果仍更接近窄街模板。

##### 2) 细节发糊、边界不干净（高频信息恢复不足）

- 路缘、建筑边线、车辆轮廓、车道线等**几何边界偏软**，存在涂抹/糊化感。
- 远处小目标（路牌、杆、细线）容易被抹平，局部结构不够清晰。

整体表现为：低频结构（大块道路/天空/建筑）较稳定，但高频细节（边界、纹理）恢复不足。

##### 3) 纹理与光照真实感不足，画面偏灰且对比度较低

- 生成图整体色调偏灰绿、对比度偏低，材质区分不明显；
- 路面颗粒、阴影、标线等纹理不够自然，呈现“块状/平滑”的质感。

这表明模型在感知层面的真实感表达有限，更偏向生成平滑的回归结果。

> **总结**：U-Net 结果较稳定，但在这两组样例中出现了明显的“输出趋同/条件利用不足”，导致语义对齐较弱；同时边界与纹理细节偏糊、真实感不足，更符合重建式生成的典型特征。

#### 7.3.3 Pix2Pix+U-Net

总体来看，Pix2Pix+U-Net 的输出呈现出**更强的语义响应与结构约束**：道路走向、可行驶区域、两侧建筑/树的布局更贴近 label 与 GT；同时仍存在**整体偏糊、局部细节缺失**的问题，但“输出趋同”的现象明显减弱。

具体总结如下：

##### 1) 语义对齐明显增强（条件约束更强）

- 两个样例的生成结果都能较好地遵循 label 的**道路形状与透视结构**：可行驶区域、道路边界、远处消失点等整体布局与输入一致。
- 相比之前那种“生成模板化街景”的情况，这里能够根据不同 label 呈现不同的场景结构，说明条件信息被更充分利用。

这符合 Pix2Pix 的特点：在条件判别约束下，生成结果基本与输入条件保持一致。

##### 2) 细节仍偏糊，小目标与高频纹理不足

- 虽然整体结构更对齐，但两个样例的生成图都存在明显的**涂抹感/低对比度**：建筑窗格、路牌、标线、树叶阴影等高频细节不够清晰。
- 小目标（远处行人、路侧标识、公交车轮廓等）仍容易被弱化或“糊成一团”，表现为“结构有了，但细节不够”。

这通常说明模型在像素一致性与感知真实感之间仍偏向较平滑的解，且高频纹理学习不充分。

##### 3) 局部伪影倾向（对抗训练常见副作用，轻微）

- 生成图中远处树冠/建筑边缘区域存在轻微的块状/糊块感，属于对抗训练中常见的局部纹理不稳定现象（强纹理约束下可能出现局部不自然）。

------

> **总结**：Pix2Pix+U-Net 能更好地利用输入 label，整体结构与语义对齐显著增强，输出不再高度趋同；但在复杂场景下仍存在细节纹理不足、边界不够锐利和小目标缺失的问题。指标上第二个样例明显优于第一个样例，说明模型在不同场景复杂度下鲁棒性仍有提升空间。

## 8. 结论与总结

（按你的结果改成确定结论）

1. 在 Label→Photo 任务中，三种模型在“像素一致性 vs 感知真实感”之间表现不同：
   - 像素指标（PSNR/MAE）：`【待填：最佳模型】`
   - 结构指标（SSIM）：`【待填：最佳模型】`
   - 感知指标（FID）与主观观感：`【待填：最佳模型】`
2. 综合考虑训练稳定性与效果，本实验推荐：`【待填：你的最终选择】`。

## 9. 可改进方向（可选）

- 损失项扩展：Feature Matching / Perceptual Loss（任务书鼓励探索不同损失项）。
- 更强生成器（如 ResNet-block / Attention U-Net）或更强判别器。
- 更高分辨率训练与多尺度判别（若算力允许）。
- 针对 FID：使用更大 batch 统计、或更严格的评估协议。

## 参考文献

- Image-to-Image Translation with Conditional Adversarial Networks (Pix2Pix, CVPR 2017)

如果你把**三套模型跑出来的指标数值（PSNR/SSIM/MAE/FID）\**和\**几张三联图**贴我（或直接发你保存的图），我可以把第 7 节的“对比分析”写成**带具体数字与结论的完整版**，并顺便帮你把段落改得更像正式实验报告写法。